name: Get Hot News

on:
  schedule:
    - cron: "33 9-20 * * *"  # 每天9:00-20:00，每小时第33分钟自动运行
  workflow_dispatch:  # 保留手动触发

concurrency:
  group: crawler-${{ github.ref_name }}
  cancel-in-progress: true

permissions:
  contents: read
  actions: write

jobs:
  crawl:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    continue-on-error: false
    strategy:
      fail-fast: true
      max-parallel: 1
      matrix:
        python-version: ["3.10"]

    steps:
      - name: Checkout repository
        id: checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: true

      - name: Retry Checkout if failed
        if: failure() && steps.checkout.outcome == 'failure'
        uses: nick-fields/retry@v3
        with:
          max_attempts: 3
          timeout_minutes: 1
          command: |
            git clone --depth=1 ${{ github.server_url }}/${{ github.repository }}.git .

      - name: Check Expiration
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          WORKFLOW_FILE="crawler.yml"
          API_URL="repos/${{ github.repository }}/actions/workflows/$WORKFLOW_FILE/runs"
          TOTAL=$(gh api "$API_URL" --jq '.total_count')
          if [ -z "$TOTAL" ] || [ "$TOTAL" -eq 0 ]; then
            echo "No previous runs found, skipping expiration check"
            exit 0
          fi
          LAST_PAGE=$(( (TOTAL + 99) / 100 ))
          FIRST_RUN_DATE=$(gh api "$API_URL?per_page=100&page=$LAST_PAGE" --jq '.workflow_runs[-1].created_at')
          if [ -n "$FIRST_RUN_DATE" ]; then
            CURRENT_TIMESTAMP=$(date +%s)
            FIRST_RUN_TIMESTAMP=$(date -d "$FIRST_RUN_DATE" +%s)
            DIFF_SECONDS=$((CURRENT_TIMESTAMP - FIRST_RUN_TIMESTAMP))
            LIMIT_SECONDS=604800
            if [ $DIFF_SECONDS -gt $LIMIT_SECONDS ]; then
              echo "⚠️  试用期已结束，请运行 'Check In' 签到续期"
              gh workflow disable "$WORKFLOW_FILE"
              exit 1
            else
              DAYS_LEFT=$(( (LIMIT_SECONDS - DIFF_SECONDS) / 86400 ))
              echo "✅ 试用期剩余 ${DAYS_LEFT} 天"
            fi
          fi

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: "pip"

      - name: Install dependencies
        uses: nick-fields/retry@v3
        with:
          max_attempts: 2
          timeout_minutes: 1
          command: |
            python -m pip install --upgrade pip
            pip install -r requirements.txt

      - name: Verify required files
        run: |
          if [ ! -f config/config.yaml ]; then
            echo "Error: Config missing"
            exit 1
          fi

      # 核心修改：运行爬虫并捕获是否有热点内容
      - name: Run crawler and check content
        id: run_crawler
        env:
          FEISHU_WEBHOOK_URL: ${{ secrets.FEISHU_WEBHOOK_URL }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
          DINGTALK_WEBHOOK_URL: ${{ secrets.DINGTALK_WEBHOOK_URL }}
          WEWORK_WEBHOOK_URL: ${{ secrets.WEWORK_WEBHOOK_URL }}
          WEWORK_MSG_TYPE: ${{ secrets.WEWORK_MSG_TYPE }}
          EMAIL_FROM: ${{ secrets.EMAIL_FROM }}
          EMAIL_PASSWORD: ${{ secrets.EMAIL_PASSWORD }}
          EMAIL_TO: ${{ secrets.EMAIL_TO }}
          EMAIL_SMTP_SERVER: ${{ secrets.EMAIL_SMTP_SERVER }}
          EMAIL_SMTP_PORT: ${{ secrets.EMAIL_SMTP_PORT }}
          NTFY_TOPIC: ${{ secrets.NTFY_TOPIC }}
          NTFY_SERVER_URL: ${{ secrets.NTFY_SERVER_URL }}
          NTFY_TOKEN: ${{ secrets.NTFY_TOKEN }}
          BARK_URL: ${{ secrets.BARK_URL }}
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
          GENERIC_WEBHOOK_URL: ${{ secrets.GENERIC_WEBHOOK_URL }}
          GENERIC_WEBHOOK_TEMPLATE: ${{ secrets.GENERIC_WEBHOOK_TEMPLATE }}
          AI_ANALYSIS_ENABLED: ${{ secrets.AI_ANALYSIS_ENABLED }}
          AI_API_KEY: ${{ secrets.AI_API_KEY }}
          AI_MODEL: ${{ secrets.AI_MODEL }}
          AI_API_BASE: ${{ secrets.AI_API_BASE }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
          S3_ACCESS_KEY_ID: ${{ secrets.S3_ACCESS_KEY_ID }}
          S3_SECRET_ACCESS_KEY: ${{ secrets.S3_SECRET_ACCESS_KEY }}
          S3_ENDPOINT_URL: ${{ secrets.S3_ENDPOINT_URL }}
          S3_REGION: ${{ secrets.S3_REGION }}
          GITHUB_ACTIONS: true
        run: |
          # 运行爬虫并记录输出
          python -m trendradar > crawler_output.log 2>&1
          # 检查是否有热点内容（关键词：新增热点、热榜、AI分析等）
          if grep -q -E "新增热点|热榜|AI分析|今日头条|百度热搜" crawler_output.log; then
            echo "has_content=true" >> $GITHUB_OUTPUT
          else
            echo "has_content=false" >> $GITHUB_OUTPUT
          fi

      # 无热点时推送「还有没有新热点」
      - name: Push empty content message
        if: steps.run_crawler.outputs.has_content == 'false'
        uses: fjogeleit/http-request-action@v1
        with:
          url: ${{ secrets.FEISHU_WEBHOOK_URL }}
          method: POST
          contentType: application/json
          data: |
            {
              "message_type": "text",
              "content": {
                "text": "还有没有新热点"
              }
            }

      # 失败通知
      - name: Notify on failure
        if: failure()
        uses: fjogeleit/http-request-action@v1
        with:
          url: ${{ secrets.FEISHU_WEBHOOK_URL }}
          method: POST
          contentType: application/json
          data: |
            {
              "message_type": "text",
              "content": {
                "text": "❌ TrendRadar爬虫任务执行失败！\n仓库：${{ github.repository }}\n运行ID：${{ github.run_id }}\n查看日志：${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
              }
            }
